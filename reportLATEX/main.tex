\documentclass[10pt]{article} % For LaTeX2e

% This template and styling file have been adapted from the ML Reproducibility Challenge 2020 and TMLR templates, to ease the submission.

% If you want to show the general instuctions, use the following command:
\usepackage[instructions]{atml}
% For the final version, use this one:
%\usepackage{atml}

% If you want to submit this work to tmlr, use the "tmlr" package rather than the "atml" one. Note: double-check the files needed in TMLR's submission instructions. 
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Note that the following lines have been added to atml.sty:
% \usepackage[hidelinks]{hyperref}
% \newcommand*{\doi}[1]{\href{http://doi.org/#1}{doi: #1}}
% If atml.sty is not used, then these lines might need to be added to the respective styling files.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{tikz}
\usepackage{subcaption}
%\usepackage[subpreambles=true]{standalone}
\usepackage{standalone}
\usepackage{multirow}
\usepackage{multicol}

\usepackage{booktabs}    
\usepackage{tabularx}
\usepackage{array}
\usepackage{float}
\usepackage[a4paper, left=0.5in, right=0.5in, top=1in, bottom=0.5in]{geometry}
\setlength{\headsep}{10pt}

\title{ATML Report}

\author{%
  \name Jonatan Bella \email \href{mailto:jonatan.bella@usi.ch}{jonatan.bella@usi.ch}
  \AND
  \name Alessia Berarducci \email \href{mailto:alessia.berarducci@usi.ch}{alessia.berarducci@usi.ch}
  \AND
  \name Jonas Knupp \email \href{mailto:jonas.knupp@usi.ch}{jonas.knupp@usi.ch}
  \AND
  \name Tobias Erbacher \email \href{mailto:tobias.erbacher@usi.ch}{tobias.erbacher@usi.ch}
}

\def\groupid{Reinforcement Learning}
\def\projectid{RLE}


\begin{document}

\maketitle

\begin{abstract}
In this work we investigate the claims of \cite{rle-paper} in the paper \href{https://arxiv.org/abs/2407.13755}{\textsc{Random Latent Exploration for Deep Reinforcement Learning}}\footnote{Please note that there also exists an older version of this paper by \cite{rle-paper-old} marked in the references with "(OLD VERSION)" which was provided by the course instructors.} which introduces a novel exploration technique for reinforcement learning. The main claim of \cite{rle-paper} is that RLE outperforms \textsc{PPO}, \textsc{NoisyNet}, and \textsc{RND} in discrete (\textsc{FourRoom}, \textsc{Atari}) and continuous (\textsc{IsaacGym}) control tasks. We replicated their experiments in the \textsc{FourRoom} environment and confirm that \textsc{RLE} outperforms the other algorithms. However, our experiments show that the performance of \textsc{RLE} depends on the choice of the latent vector distribution, which contradicts the findings of \cite{rle-paper}. Furthermore, we replicated their experiments for the two \textsc{Atari} games \textsc{Alien} and \textsc{StarGunner}. While RLE outperformed all baselines in \textsc{Alien}, it performed worse than \textsc{PPO} but better than \textsc{NoisyNet} and \textsc{RND} in \textsc{StarGunner}, contrasting with the original results of \cite{rle-paper}, where RLE dominated both games. \cite{rle-paper} report the performance of \textsc{RLE} in the \textsc{IsaacGym} enviornment \textsc{CartPole}. Since \textsc{IsaacGym} is deprecated we tested RLE in the \textsc{CartPole} environment of \textsc{IsaacLab} where it outperformed the two baselines. Overall, we can confirm that \textsc{RLE} is a suitable alternative to existing exploration-focued reinforcement learning algorithms. Lastly, we implemented two \textsc{RLE} variants with adaptive latent vector distribution and tested it in the \textsc{Alien} game where both variants outperformed standard \textsc{RLE}.


This report is prepared as part of the course project in \textit{Advanced Topics in Machine Learning} at \textit{Universit√† della Svizzera italiana} in the autumn semester of 2024. 
\end{abstract}

%%%%

\section{Introduction}
\noindent The paper we investigate deals with the problem of motivating an agent in a high-dimensional state space to explore the environment more exhaustively during training and thereby find non-obvious trajectories that can lead to higher long-term rewards for both discrete and continuous action spaces. The paper compares the new \textit{Random Latent Exploration} (RLE) technique to standard \textit{Proximal Policy Optimization} (PPO, see \cite{ppo-paper}), \textit{NoisyNet} (see \cite{noisynet-paper}) and \textit{Random Network Distillation} (RND, see \cite{rnd-paper}).

\noindent Exploration is one of the major challenges of Reinforcement Learning (RL) and its techniques can generally be divided into two categories: Noise-based exploration and bonus-based exploration. There are advantages and disadvantages for both of them but we always have to keep in mind that always choosing the highest short-term (local) reward does not necessarily also yield the highest long-term (global) reward. E.g., picture the environment shown in figure \ref{fig:sample-env-1}. Here, the agent starting in the \textcolor{blue}{blue state} will always choose the small reward of $1$, i.e. going right, then move back to the initial state, then move right, and so on, dithering between these two states, instead of accepting to collect a reward of $0$ by going left in order to be able to collect the much higher reward of $100$ in the following step.

\noindent \cite{rle-paper} use RND to represent bonus-based exploration, NoisyNet to represent noise-based exploration and standard PPO as the baseline benchmark.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tikz/sample-env-1.tex}
    \caption{Pure rewards.}
    \label{fig:sample-env-1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tikz/sample-env-2.tex}
    \caption{Rewards with randomization (example).}
    \label{fig:sample-env-2}
  \end{subfigure}
  
  \caption{An exemplary environment consisting of four state where transitions can occur between neighboring states. The \textcolor{blue}{blue node} is the initial state and the numbers are the rewards.}
  \label{fig:sample-env}
\end{figure}

\newpage
\section{Scope of reproducibility}
\label{sec:claims}

\noindent The main quantifiable claims made by \cite{rle-paper} that we will be testing consist of the following:

\begin{enumerate}
    \item In the abstract, \cite{rle-paper} claim "[...] RLE exhibits higher overall scores across all of the tasks than other approaches, including action-noise and randomized value exploration", which they later specify for both "[...] discrete and continuous control tasks." They refer to the tasks being the \textsc{Atari}, \textsc{IssacGym} and \textsc{FourRoom} environments. The benchmark approaches are standard PPO, NoisyNet and RND.
    \item Moreover, "[...] introducting randomness to rewards influences the [... agent] to produce diverse behaviors" \citep{rle-paper} when this randomness is used to condition the policy and value networks, which manifests itself in an incentive to explore random (and thus in aggregate lager) parts of the state space.
    \item For the discrete-action discrete-states \textsc{FourRoom} environment absent of a goal reward, the authors claim that PPO's state visitation centers around the initial room (top right) concluding that action-noise methods cannot conduct deep exploration whereas RLE, NoisyNet and RND well reach across the four rooms and thus perform deep exploration.
    \item RLE performance improves over PPO irrespective of the distribution used to sample the 8-dim random latent vector.
\end{enumerate}

\noindent Our findings regarding these claims will be presented in \hyperlink{sec4}{section 4} \textcolor{red}{Check link before submission}. Please note that there are more claims made by the authors, however, due to time constraints we are not able to test all of them, the remaining ones are shown in \hyperlink{untested-claims}{appendix B} \textcolor{red}{check link before submission.}

\section{Methodology}
Random Latent Exploration --- The new idea that the authors \cite{rle-paper} present is to augment the reward function by adding a randomized term which incentivizes the agent to explore a larger portion of the state space. In particular, we will call this term $F(s, z)$ or intrinsic reward function, where $s \in \mathcal{S}$ is any state of the state space $\mathcal{S}$, and $\textbf{z} \in \mathbb{R}^{d}$ is a $d$-dimensional vector sampled from a given distribution $P_{\textbf{z}}$. The authors \cite{rle-paper} claim that RLE's efficiency is not significantly affected by the choice of $P_{\textbf{z}}$. If at time step $t \in \{0, 1, 2, ..., T\}$ the agent in state $s_{t}$ takes action $a_{t} \sim \pi(. | s_{t})$ from policy $\pi$, then it will obtain the task reward $r (s_{t}, a_{t})$. In RLE, we train a so-called latent-conditioned policy network $\pi(. | s, \textbf{z})$ and latent-conditioned value network $V^{\pi}(s, \textbf{z})$ to estimate and then maximize the expected sum of rewards from a given state, aware of the random term $z$: $V^{\pi}(s, \textbf{z}) \approx \mathbb{E}_{\pi} \left[\sum_{t = 0}^{\infty} \gamma^{t} \left(r(s_{t}, a_{t}) + F(s_{t+1}, \textbf{z})\right) \right]$

\noindent In the equation above, $\gamma$ describes the discount factor and $F(s_{t+1}, \textbf{z}) =  \boldsymbol{\phi}(s) \cdot \textbf{z}$ where $\boldsymbol{\phi}(s): \mathcal{S} \rightarrow \mathbb{R}^{d}$ is a feature extraction network. The feature network is updated as a linear combination of the old feature network's weights and the weights of the value network\footnote{In the pseudocode algorithm in line 19 on page 25, in the mathematical formulation, the authors state that the feature network is updated as a linear combination of the old feature network's weights and the policy network's weights. We contacted the authors, and they confirmed that it should read $\phi \leftarrow \tau \cdot V^\phi + (1 - \tau ) \cdot \phi$ (i.e., the value network should be used, not the policy network). However, they state it would be interesting to explore which network should be used to update the feature network.}. Both $\boldsymbol{\phi}$ and $\textbf{z}$ are $d$-dimensional vectors. Note that although the value-equation is presented by \cite{rle-paper}, in the implementation they introduce an intrinsic and extrinsic reward coefficient. In the experiments they choose intrinsic reward coefficient $\ll$ extrinsic reward coefficient. Thus, they are not directly optimizing the value-equation in the experiments. Furthermore, on page 3 the authors state that $\vz$ "is resampled at the start of each trajectory". However, in the pseudocode on page 15 the authors note that $\vz$ is resampled at the end of each trajectory or after a certain number of steps has passed. Since the authors implemented the latter, more general, case we adopt the latter approach for our experiments. The authors split the critic's head into two: one head predicts the intrinsic value function, the other head predicts the extrinsic value function. Figure \ref{fig:rle-architecture} depicts how the action logits, intrinsic value, and extrinsic value for a given observation and $\vz$ is calculated. Note the residual connection from the first element-wise addition to the second element-wise addition. There is no mention of this residual connection in the paper. However, it is present in the provided code for the \textsc{Atari} environment. To continue the example from the introduction, in figure \ref{fig:sample-env-2} the agent would prioritize to go left as this promises the greater reward. It is not hard to imagine that for higher dimensional environments, every time we start a new trajectory and correspondingly sample a new $z$, the agent will explore a different region of the state space and thus we can discover non-obvious paths to maximize the rewards. For the detailed pseudocode of this algorithm, see \hyperlink{algo-rle}{Appendix A} \textcolor{red}{(check link before submmission)}.

\noindent A description of the other three algorithms can be found in \hyperlink{algo-ppo}{Appendix B} (PPO), \hyperlink{algo-noisynet}{Appendix C} (NoisyNet) and \hyperlink{algo-rnd}{Appendix D} (RND). Moreover, since the architecture of the neural networks we are using, e.g. for the feature extractor, value function approximation and policy, differ for each environment and every algorithm, we will explain them in \hyperlink{experimental-setup}{section 3.4} in detail. \textcolor{red}{(check all four links before submission)}

\hypertarget{hyperparameter-subsection}{\subsection{Hyperparameters}}

\noindent Wherever possible we used the same hyperparameters as the authors, except for $z$-resampling frequency in the \textsc{Atari} games. For the parameters that were missing a description in the paper we adopted values that seemed reasonable to us, which you can find in the \hyperlink{appendix-hyperparams}{appendix C}.

\hypertarget{experimental-setup}{\subsection{Experimental setup and code}}

\noindent The code for our implementations can be found in GitHub\footnote{\href{https://github.com/jonupp/adv_topics_ml_repl_chal}{https://github.com/jonupp/adv\_topics\_ml\_repl\_chal} \textcolor{red}{Change github}}. There are three types of environments in use on which we run our RL algorithms which are depicted in figure \ref{fig:environmnents}. 


\noindent The authors of the paper at hand published code in a GitHub repository\footnote{\href{https://github.com/Improbable-AI/random-latent-exploration}{https://github.com/Improbable-AI/random-latent-exploration}}. This repository contains code for the \textsc{Atari} and \textsc{IsaacGym}\footnote{Please note that the \textsc{IsaacGym} environment is deprecated and we are using \textsc{IsaacLab} instead.} environments with the 4 algorithms (RLE, PPO, RND, NoisyNet). The authors used the PPO, NoisyNet, and RND implementations from \cite{clearnrl-paper}.

\subsubsection{Atari}

\noindent The authors of the paper under investigation chose the \textsc{Atari} environment, introduced by \cite{atari-introduction}, to represent a discrete action-space deep-RL benchmark. This environment comprises $57$ different games, among which we performed experiments on two games, namely \textsc{Alien} and \textsc{StarGunner}. To understand the code given by the authors, we made use mainly of the official environment documentation as well as other papers that we found along the way. Here, the agent perceives the $4$ most recent $84\times84$ grayscale frames, one frame of which can be found in figure \ref{fig:sample-env-atari}. During training, every trajectory (episode) always commences from the same initial state and can be terminated in one of the two ways: A game event occurs, or the maximum episode length is reached. The former refers to e.g. the loss of a "life" or a task being completed. As the authors provided the code for this environment, we did not modify it significantly. The main claims that we tested here were the superior performance of RLE and deeper exploration.\\

\noindent The game score $S_{G}$ that we use to compare the different algorithms we run in this environment are normalized in the manner introduced by \cite{agent57} by taking into account the score an average human achieves $S_{H}$ as well as the score an agent achieves by always choosing a random action $S_{R}$, which gives us the human normalized score $S_{N} = \frac{S_{G} - S_{R}}{S_{H} - S_{R}}$. According to \cite{agent57}, in the case of \textsc{Alien}, we have $S_{H} = 7127.7$ and $S_{R} = 227.8$ while for \textsc{StarGunner} we have $S_{H} = 10250$ and $S_{R} = 664$. With the normalization we can get a better comparison of how much the agent behaves like a random agent ($S_{N} = 0$) or like a human player ($S_{N} = 1$), or even better $S_{N} > 1$. Moreover, the normalization allows for comparisons between games, adjusted for the general difficulty of a game.\\

\noindent Each game was run with all four algorithms (RLE, PPO, NoisyNet, RND) and the most important comparative measure to evaluate the agent's performance is the game score, which was computed from the cumulative extrinsic rewards (obtained from $\text{infos["r"]}$ of the environment) over the episodes, treated as undiscounted sums to ensure a uniform weight across time steps for the purpose of investigating the algorithm's exploratory abilities. To this end, we also record the time evolution of the Shannon entropy of each algorithm, where a higher entropy indicates that the agent's selection of actions out of all available actions is broader and less concentrated, hinting at more thorough exploration capabilities.

\subsubsection{FourRoom}
Figure \ref{fig:sample-env-fourroom} shows the \textsc{FourRoom} environment where we have $50\times50 + 4$ states\footnote{In \cite{rle-paper}, the authors claim that there are $50\times 50$ states. We contacted the authors, and they confirmed that it should read $50\times 50 + 4$ states.} which are divided into four rooms of size $25\times25$ and $4$ holes in the walls located at positions $10$ horizontally and $5$ vertically, starting the count at $1$ from the inner corners. Exploring the state space, beginning from the initial state marked in \textcolor{red}{red}, becomes a deep exploration task due to the holes being only $1$ "pixel" wide. However, if we want to incentivize the agent to find a reward, we can add a goal state marked in \textcolor{green}{green}. The \textsc{FourRoom} environment was implemented such that we can pass a flag to the environment's constructor to choose whether the environment is reward-free or has the goal in the bottom-left corner. At every step, the agent can move one step vertically or horizontally, but if it would run into a wall it chooses to remain in place. \cite{rle-paper} refer to \cite{grid-world-paper} when introducing the \textsc{FourRoom} environment. \cite{grid-world-paper} use action stochasticity. We asked Mr. Mahankali whether they also used action stochasticity, and they stated that they did not use action stochasticity in their experiments. 

The authors did not provide any code for the \textsc{FourRoom} environment, so this environment was implemented by us using the gymnasium library introduced by \cite{gymnasium-paper} together with our adaptation of the code for PPO, NoisyNet and RLE algorithms from the \textsc{Atari} environment, while we adapted the RND code from the \textsc{Isaac} environment. A wrapper to record the state visitation counts per environment was also implemented. Before we adapted RLE from the existing implementation for the \textsc{Atari} environment, we implemented it based on the description in \cite{rle-paper}. However, the paper lacked several details present in the \textsc{Atari} code (e.g., split critic's head, residual connection) so that we decided to proceed with the adapted code to have consistent results over the different environments. 

First we investigate how RLE performs in a reward-free setting in comparison to PPO, NoisyNet, and RND. \cite{rle-paper} claims that RLE shows good explorative behavior based on a single heatmap of state visitation counts. To assess whether this result is expected or exceptional we trained each algorithm 20 times. Since it is not reasonable to compare 20 state visitation heatmaps we used the Shannon entropy, introduced by \cite{shannon-entropy-paper}, of the state visitation counts as a proxy for how well an agent explored the environment. With increasing entropy the distribution of the state visitation counts becomes more uniform. Thus, the higher the entropy the better the explorative behavior. 

We also ran RLE, PPO, NoisyNet, and RND in the \textsc{FourRoom} environment with a goal to see how RLE compares to the other algorithms in an environment with a reward. Similar to the reward-free setting, we run each algorithm 20 times on different seeds. However, instead of assessing the performance of the algorithms using the Shannon entropy, we use the average game score. The game score is the average undiscounted return over the last 128 episodes. The average game score for a certain algorithm is the average of the game scores in the different runs for this algorithm.

Furthermore, it was explored if the intrinsic reward function really guides the generation of diverse trajectories in a reward-free environment. To assess this, the intrinsic reward function for four latent vectors $z$ and four trajectories generated using the same latent vector $z$ were saved at the end of each experiment. 

We also tested to what degree RLE is indifferent to the distribution of the latent vector. To test this, we ran RLE with different latent vector distributions. The following distributions were considered: standard normal distribution, standard uniform distribution, Von-Mises distribution with $\mu=0$ and $\kappa=0.3$, and the exponential distribution with $\lambda=0.3$. We ran 20 experiments for each latent vector distribution and recorded the state visitation entropy. The same hyperparameters were used as before. Note that all latent vectors were normalized using the L2-norm, so that all latent vectors are on a unit sphere around the origin. This ensures that all intrinsic rewards are in the interval [-1,1]. \cite{rle-paper} performed a similar ablation study for the \textsc{Atari} environment. However, they did not normalize the latent vectors for all distributions. They only applied normalization for the normal distribution which they then denoted as "Sphere". They then compare the "Sphere" to the unnormalized uniform distribution and the unnormalized normal distribution. We normalized all distributions because the extrinsic and intrinsic reward coefficient hyperparameters already control the weighting between the intrinsic and extrinsic rewards, so we wanted the intrinsic rewards to be in the interval [-1,1].


\subsubsection{IsaacLab}
%\textcolor{red}{@Jonathan explain environment and experiments (NOT RESULT)}
In the case for this implementation, the code provided was develop in IsaacGym, which is deprecated.\footnote{The poor documentation for migrating from IsaacGym to IsaacLab provided by NVIDIA: \url{https://isaac-sim.github.io/IsaacLab/main/source/migration/migrating_from_isaacgymenvs.html}} Therefore, we decided to reimplement it in IsaacLab which is a 
reinforcement learning framework built on top of IsaacSim. However, the are changes in terms of the code architecture and features of the software. As an example, 
the physics engine is an improved version with respect to IsaacGym, with different asset extensions.

Therefore, for implementing the paper we based in an example provided by \href{https://skrl.readthedocs.io/en/latest/#}{SKRL}.
Using their based implementation for PPO as an example, we could identify the main structure to develop our own version. 

I will proceed to describe each component of the implementation. The \textsc{environment} file (\url{cartpole\_rle\_env.py}) extends the basic Cartpole environment class with the RLE necessary components as for example, 
the latent vector generation and the intrinsic and extrinsic rewards. The configuration file (\url{ppo\_cfg\_rle.yaml}) contains the hyperparameters for the training process. The \url{models.py} file contains the policy and value networks
implementation for the rle case. Whereas, the \url{rle\_network.py} file contains the feature network implementation for the latent vector generation. 
Besides these files, we needed to implement the trainer file (\url{train\_rle.py}) which handles the arguments - acting as entry point, initialize IsaacSim, setup up the trackings, configures the environment and 
instantiates the runner (\url{runner\_rle.py}) which instantiates the agent, coordinates the interactions between the agent and the environment and executes the training. Finally, the PPO-RLE algorithm (\url{ppo\_rle\_sk.py})
which is the custom PPO that handles the dual rewards for the two heads of the value network, the generalized advantage estimation and the learning scheduling among others. 

For the development of each of the parts we did not relay in the authors code unless to check the default parameters when they were not stated in the paper and as a double check of the logic of the optimization since the 
complexity of the environment interaction with Isaaclab is higher and was mostly managed by modifying deeply the skrl base ppo code. Since this implementation are highly strucuted we needed to implement work arounds, as for example,
this algorithm has implemented the actor - critic structure handling but not a feature network, therefore we handle this by generating the latent vector in the environment, computing the intrinsic rewards and passing it through the runner 
to the modified ppo algorithm to handle the dual rewards and the soft update of the feature network during policy updates and environment resets. In addition, the maximum number of steps is around 299 for episode in this implementation of
IsaacLab using the skrl base libraries, however, the authors have a maximum step of 500. Since increase the step would require higher amount of computations plus library restrictions we opted to keep the 300 steps which means that our results are
a scaled down version of the authors but with relative the same results. 

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/atari.png}
    \caption{The \textsc{Alien-v5} environment is an example of the \textsc{Atari} games consisting of a maze filled with 'eggs' to destroy while being hunted by aliens. A flamethrower or occasional power-up may be used to scare the aliens.}
    \label{fig:sample-env-atari}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tikz/fourroom-env.tex}
    \caption{The \textsc{FourRoom} environment is a grid world, limited by walls, with the initial state is on the \textcolor{red}{top right} and the goal state where the agent receives a reward (optional) is on the \textcolor{green}{bottom left}.}
    \label{fig:sample-env-fourroom}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/isaac.png}
    \caption{The \textsc{CartPole} environment is an example of the \textsc{IsaacLab} environment.\\\textcolor{red}{What does the agent actually perceive?}\\}
    \label{fig:sample-env-isaac}
  \end{subfigure}
  \caption{Examples of the environments in use.}
  \label{fig:environmnents}
\end{figure}


\subsubsection{Queue-Based Von Mises-Fisher}
\cite{rle-paper} suggest it might be worthwhile to explore using a latent vector distribution that changes during the training. Our hypothesis is that at the start of the training the latent vector distribution $P_{\textbf{z}}$ should provide diverse $\textbf{z}$ since the agent should explore the environment. However, with increasing training progress, the agent should put less emphasis on exploration. We use the Von Mises-Fisher distribution with the parameters $\vmu$, the mean direction, and $\kappa$, the concentration to model $P_{\textbf{z}}$. The goal is to gradually adjust $\vmu$ during training so that it converges to a point in the space where $P_{\textbf{z}}$ generates $\mathbf{z}$ samples that consistently produce high returns. Furthermore, $\kappa$ should be slowly increased during the training to increase the probability to sample $\mathbf{z}$ close to $\vmu$. Since PyTorch does not provide an implementation to sample from this distribution, we used the implementation described by \cite{von-mises-fisher-paper} that is based on rejection sampling. We maintain a queue of $\textbf{z}$ values with good episodic return and a queue with the respective returns. After every finished episode, the queues are updated if necessary. We then update $\vmu$ as a linear combination of the $\textbf{z}$ values in the success memory. The weighting is done using the returns in the returns memory. $\kappa$ should be large if the model has converged to a $\vmu$ that reliably gives $\textbf{z}$ values with high returns. Thus, $\kappa$ is updated by linearly interpolation between 0 and 30. The weight is the average pairwise cosine similarity between the $\textbf{z}$ values in the success memory.


\subsubsection{Neural Adaptive Von Mises-Fisher}

We explore further on the idea of using a distribution that allows for a natural manipulation of the exploration-exploitation trade off. Since steps of an episode have a temporal dependency, therefore, we thought as a good approach to process the trajectories with an LSTM, see figure \ref{fig:neural-adaptive-vmf}. When a new episode ends, the hidden state is reset since between episodes the agent should not keep any information, however, the actual knowledge encoded in the weights of the LSTM are preserved. 

After the LSTM pre-process the states features generated by the agent network, the mu network receives these processed features to predict a good direction of exploration. However, it could have different degrees of confidence on the given direction and features, reason why a kappa network will predict the kappa value that represents the confidence on the predicted mu for the given states in the trajectory. Once kappa and mu are obtained, we can sample from the VMF distribution to get the suggested direction of the next action to be taken. Keeping the sampling allows for a degree of exploration that cannot be represented by a non-stochastic output. 

The obtained z, in conjunction with the processed features, are fed into the reward network that acts as the dot product in the paper's RLE network. Therefore, the reward network will operate with the respective features and the z vector to produce an intrinsic reward to guide exploration.

For the construction of the losses we refer to \hyperlink{na-vmf}{appendix H}. \textcolor{red}{check link}

\hypertarget{computational-requirements}{\subsection{Computational requirements}}

\noindent All \textsc{FourRoom} experiments were performed on an Apple M2 Pro with 16 GB RAM. Every experiment was conducted in the reward-free environment and in the environment with a goal. Every algorithm and every RLE variant with different latent vector distribution was run 20 times with different seeds in each of the two environments. This resulted in a total of 280 runs. It took ca. 1 minute to do a single PPO run, ca. 1 minute and 20 seconds to do a single NoisyNet run, ca. 2 minutes to do a single RLE run, and ca. 2 minutes and 20 seconds to do a single RND run. The experiments for the different algorithms took at total time of ca. 4.4 hours. In addition, we trained RLE with three different latent vector distributions (apart from the standard normal distribution). These experiments took ca. 4 hours. Thus, the total computing time for all \textsc{FourRoom} experiments amounts to ca. 8.4 hours.

\noindent All experiments in the \textsc{Atari} environment were run in \href{https://colab.research.google.com/}{Google Colab}. Here we experimented with the available CPU, T4 GPU and TPU v2-8 and used Weights \& Biases (W\&B) for a real-time visualization of the game scores and other measures. In the end we chose the GPU for all runs. We linked a \href{https://drive.google.com}{Google Drive} to Colab in order to save model checkpoints. Overall the W\&B workspace now contains 56 runs with a total compute time of ca. $241.2$ hours, of which we are using 8 runs for the results of this project. Of that amount, ca. $76.7$ compute hours were spent on \textsc{StarGunner}, the remaining ca. $164.5$ compute hours were spent on \textsc{Alien}.

%\noindent \textcolor{red}{@Jonatan: We need your input here for Isaac: Did you run all of the experiments on your mac/other laptop or did you use colab as well (which CPU or GPU was used (name))? What are the average training times e.g. per episode or global step? How many runs did you do overall (i.e. how many compute hours did you spend on this project) for each of the algorithms?}

With respect to IsaacLab, the experiments were run on an Asus TUF Gaming A17 with a RTX 3060 Laptop GPU. The GPU characteristics are below 
the suggested minimum requirements for VRAM memory of NVIDIA, however, for developing the experiments for the CartPole environment was enough.
The used CUDA version was 11.8 and the project packages were managed through conda besides the separate installation of Omniverse for IsaacSim and 
the posterior IsaacLab installation (Windows distribution). 

\noindent We would like to note, however, that \cite{rle-paper} had the HPC resources of the MIT Supercloud and the Lincoln Laboratory Supercomputing Center available. Within the framework of this course, we did not have the equivalent computing power to replicate all experiments in their original form and had to restrain ourselves to a mere few of them.

\hypertarget{sec4}{\section{Results}}

\noindent In this section we present the results of our experiments.
%\textcolor{red}{all figures in appendix as first item}

%\noindent \texttt{Start with a high-level overview of your results. Do your results support the main claims of the original paper? Keep this section as factual and precise as possible, and reserve your judgment and discussion points for the next "Discussion" section.}


\subsection{Results reproducing original paper}
%\texttt{For each experiment, say (1) which claim in Section~\ref{sec:claims} it supports, and (2) if it successfully reproduced the associated experiment in the original paper.}
%\texttt{For example, an experiment training and evaluating a model on a dataset may support a claim that that model outperforms some baseline.}
%\texttt{Logically group related results into subsections.}

\subsubsection{FourRoom}
The results from the experiment where every algorithm is run 20 times in the reward-free environment can be seen in figure \ref{fig:gridworld-entropy-algorithms}. RLE shows the best performance, followed by RND, NoisyNet and PPO. Furthermore, RLE has the narrowest confidence interval. Although \cite{rle-paper} did not perform an equivalent experiment, they concluded from the state visitation heatmap that RLE shows the best explorative behavior in this setting. This aligns with the result from our experiment. 

Figure \ref{fig:gridworld-score-algorithms} shows the result of the experiment where every algorithm is run 20 times in the environment with a goal. \cite{rle-paper} train five seeds in the environment with a goal for each algorithm, and find that the average score for RLE and NoisyNet is 0.6, while the average score for RND and PPO is 0. In our experiment RLE achieved the highest game score by a large margin while NoisyNet and RND performed similarly poorly with an average game score of $<0.2$ at the end of the training. PPO achieved an average game score of 0 at the end of the training. Thus, the result that RLE is among the best performing algorithms in this setting is confirmed by our experiment.

Figure \ref{fig:gridworld-entropy-distributions} depicts the result of the experiment where RLE variants with different latent vector distributions are compared in the reward-free environment by running each RLE variant 20 times. The RLE variants where the latent vector distribution follows the standard normal or Von Mises distribution perform similarly well and clearly outperform the RLE variants with standard uniform and exponential distribution. Moreover, the RLE variants with standard uniform and exponential distribution show similar performance to RND. These results indicate that the choice of the latent vector distribution is of some importance. Moreover, there are two pairs of distributions that show similar behavior although the distributions in the pair are not that similar. For instance, we chose parameters for the Von Mises distribution to make its peak small, so that it is quite different from a standard normal distribution and shows some similarity to a uniform distribution. 

In figure \ref{fig:gridworld-score-distributions} the results of the experiment where RLE variants with different latent vector distributions are compared in the environment with rewards. Again, each RLE variant was run 20 times. The RLE variant with the Von Mises distribution performs best, followed by the standard normal variant, the exponential variant, and lastly the standard uniform variant. The difference in average game score between the RLE variants is large: the worst variant achieves an average game score slighlty above 0.2 while the best RLE variant achieves an average game score of appr. 0.8. The ranking of the RLE variants is the same as in the experiment in the reward-free environment. Again, the results indicate that the choice of the latent vector distribution impacts the performance of RLE.

We generated state visitation heatmaps for each algorithm and RLE variant for both the reward-free environment and the environment with a goal. From the 20 runs for each algorithm and RLE variant we chose the run with the highest entropy in the case of the reward-free environment and the run with the highest game score in the case of the environment with a goal. In figure \ref{fig:gridworld-heatmap-nogoal-algorithms} the state visitation heatmaps for the different algorithms in the reward-free environment are displayed. RLE and NoisyNet explore all four rooms while PPO and RND have a significant number of unexplored states. Figure \ref{fig:gridworld-heatmap-goal-algorithms} shows the state visitation heatmaps for the different algorithms in the environment with a goal. RLE, NoisyNet, and RND are frequently visiting states on an appr. shortest path from the start to the goal while PPO has only explored a single room. Next, the state visitation for the RLE variants are considered. In figure \ref{fig:gridworld-heatmap-nogoal-distributions} we see that the standard normal, Von Mises, and exponential RLE variants explore all the four rooms well. Only the standard uniform RLE variant does not explore all states. In figure \ref{fig:gridworld-heatmap-goal-distributions} we see that all RLE variants often visit states on one of the appr. shortest path between start and goal. However, the standard normal and Von Mises variants also explore almost the entire state space while the standard uniform and exponential RLE variants have large unexplored chunks.

Lastly, figure \ref{fig:gridworld-value-function-trajectories} shows that the intrinsic reward function guides the generation of diverse trajectories. The trajectories converge to areas where the intrinsic reward is large. 

All the 95\% confidence intervals are calculated using the bootstrapping method. 

\subsubsection{Atari}

\noindent After running each game for a total of $40$ million time steps, we have obtained the results shown in table \ref{tab:atari-results}.

\begin{table}[h!]
  \centering
  \caption{Results of the \textsc{Atari} experiments.}
  \begin{subtable}[h]{0.48\textwidth}
      \centering
      \begin{tabular}{{llll}} 
        \hline
        \textbf{Algorithm} & & Game Score & Human-Norm.\\
        \hline
        RLE & & $1626$ & $0.203$\\ 
        PPO & & $1384$ & $0.168$\\
        NoisyNet & & $648$ & $0.061$\\
        RND & & $1048$ & $0.119$\\
    \end{tabular}
    \caption{The \textsc{Alien} game.}
    \label{tab:alien-score}
  \end{subtable}
  \hfill
  \begin{subtable}[h]{0.48\textwidth}
      \centering
      \begin{tabular}{{llll}} 
        \hline
        \textbf{Algorithm} & & Game Score & Human-Norm.\\
        \hline
        RLE & & $42481$ & $4.362$\\ 
        PPO & & $54564$ & $5.623$\\
        NoisyNet & & $27530$ & $2.803$\\
        RND & & $21565$ & $2.180$\\
    \end{tabular}
    \caption{The \textsc{StarGunner} game.}
    \label{tab:stargunner-score}
  \end{subtable}
  \label{tab:atari-results}
\end{table}

\noindent For the \textsc{Atari} game, as can be seen in table \ref{tab:alien-score}, the algorithm perform in a similar manner to the authors' result. Only NoisyNet performs much worse in our run, obtaining a score of $648$, while the authors report a score of approximately $1113$. However, regarding claim $(1)$ we can confirm that RLE obtains the highest score of them all, which is evident from the plot of human-normalized scores in figure \ref{fig:alien-score}. Additionally, we can compare the depth of exploration done by these algorithms in figure \ref{fig:alien-entropy} by looking at the entropy distribution. Here, it is evident that RLE explores the largest part of the state space in comparison to the other algorithms. Using entropy as a measure of diverse behavior, we can moreover confirm as well claim $(2)$ for this game.\\

\noindent In the case of the \textsc{StarGunner} game, as becomes apparent in table \ref{tab:stargunner-score}, the RLE algorithm does not yield the best game score but PPO does. This result differs strongly from the findings of the authors. In more detail, we find that RLE obtains a game score of around $42481$ while the authors report $64011$, and in addition their implementation of NoisyNet performs much better at $42645$ while our agent only got $27530$ in the end. Thus, in this case we have to object claim $(1)$, which is shown in more detail in figure \ref{fig:stargunner-score}. On the contrary, we can confirm claim $(2)$ again as shown in figure \ref{fig:stargunner-entropy} since RLE does exhibit the most diverse behavior out of all four algorithms. Perhaps this hints at instabilities in the algorithmic behavior of the agent which are depending on e.g. the seed with which the run was initialized.

\subsubsection{IsaacLab}
The results from our IsaacLab replication show similar results to the authors'. The RLE algorithm shows the most stable performance in the CartPole environment, see figure \ref{fig:cartpole-return} which replicates
figure 28 and 22 of the paper. The difference on the maximum score comes from the maximum length of the episodes which are kept in 299 steps in out replication for computational reasons, besides that is not directly manipulable 
in the environment wrapper as on previous IsaacGym. Notice that we also use the same hyperparameters as the authors' implementation for PPO, under the default set of parameters of skrl the results can differ from the authors.

\subsection{Results beyond original paper}
%\texttt{Often papers don't include enough information to fully specify their experiments, so some additional experimentation may be necessary. For example, it might be the case that batch size was not specified, and so different batch sizes need to be evaluated to reproduce the original results. Include the results of any additional experiments here. Note: this won't be necessary for all reproductions.}

\subsubsection{Atari}

\noindent One interesting observation that we made during our experiments is the fact that in \textsc{Alien}, the agent seems to obtain lower rewards on average with RLE, see figure \ref{fig:alien-rewards}, but still has the highest game score overall. We conjecture that this means the agent with RLE behaves more cautiously trying to stay alive for longer and collect lower/safer rewards but over a longer time horizon meaning the overall accumulated reward (game score) still turns out to be superior compared to the other algorithms. This would be confirmed by RLE having the largest trajectory (episode) length, see figure \ref{fig:alien-trajectorylength}. However, this is probably not the entire explanation, since in \textsc{StarGunner}, see figure \ref{fig:stargunner-rewards} and \ref{fig:stargunner-trajectorylength}, RLE also gets very low reward on average but here this cannot be compensated by having the largest trajectory length. Moreover, given that we also normalized the game scores, the result suggests that the agent is better at playing \textsc{StarGunner} and worse at playing \textsc{Alien} compared to an average human.

\noindent Moreover, as mentioned in the \hyperlink{hyperparameter-subsection}{hyperparameter overview}, we increased RLE's z-sampling frequency from every 1280 time steps (authors) to every 500, but we did not find materially different results. Another observation we made is that for some reason in the RLE implementation of \textsc{Alien}, the results different drastically between using a GPU and a CPU/TPU for the computations, but the underlying cause of this is beyond our understanding.

\subsubsection{Adaptive Von Mises-Fisher}
The observed results from the adaptive VMF are promising (see figure \ref{fig:vmf-game-score}), with the following comparable scores: 

\begin{table}[h!]
  \centering
  \caption{Results of the \textsc{Atari} experiments with Adaptive VMF}
  \begin{subtable}[h]{0.48\textwidth}
      \centering
      \begin{tabular}{{llll}} 
        \hline
        \textbf{Algorithm} & & Game Score & Human-Norm.\\
        \hline
        QVMF & & $2184$ & $0.285$\\
        NAVMF & & $2109$ & $0.273$\\ 
        RLE & & $1626$ & $0.203$\\ 
        PPO & & $1384$ & $0.168$\\
    \end{tabular}
    \caption{\textsc{Alien-V5}}
    \label{tab:alien-score-vmf}
  \end{subtable}
  \label{tab:atari-results-vmf}
\end{table}

In addition, trajectories were the longest for NA-VMF and equally long with respect to RLE for QVMF, see figure \ref{fig:vmf-trajectory-length}. This implies that 
in the Q-VMF case the agent was able to kill more aliens relative to the other algorithms. Therefore, both of our adaptive VMF implementations outperforms the other algorithms for 
\textsc{Alien-V5}, pointing us to further investigate this approach for other \textsc{Atari} games and continuous action spaces. 

Furthermore, we keep the same resampling frequency as in RLE, however, for adaptive VMF the loss terms, as explained, can naturally provide the agent with the feedback needed to resample z's in the appropriate moment, 
which can be not fixed a priori since many decisions change on the state context. 

\section{Discussion}
Most of our experiments confirm claim 1. \textsc{RLE} outperforms the baselines in the \textsc{FourRoom} and \textsc{IsaacLab} environments. Only in \textsc{Atari} we got mixed results. While \textsc{RLE} outperformed the baselines in \textsc{Alien} it did not manage to outperform PPO in \textsc{StarGunner}. Due to computation and time constraints we could only do experiments on 2 out of 57 \textsc{Atari} games and 1 out of 9 \textsc{IsaacLab} environments. 

Regarding claim 2: in the \textsc{FourRoom} experiments we could see that \textsc{RLE} explored a larger portion of the state space than the baselines. From the diverse trajectories generated by \textsc{RLE} we conclude that \textsc{RLE} is able to produce diverse behavior. In the two \textsc{Atari} environments \textsc{Alien} and \textsc{StarGunner} the action entropy of \textsc{RLE} was higher than the action entropy of the baselines. This hints that \textsc{RLE} shows more diverse behavior than the baselines. 

Claim 3 was investigated in the \textsc{FourRoom} experiments. In our experiments \textsc{RLE} generated diverse trajectories so that the agent could explore all the four rooms.

Our \textsc{FourRoom} experiments do not support claim 4. The performance of \textsc{RLE} with the standard normal and von mises latent vector distribution performed better than the standard uniform and exponential latent vector distributions. However, \cite{rle-paper} did the experiments with different latent vector distributions in \textsc{Atari} environments. They report that \textsc{RLE} robust with respect to the latent vector distribution.

Furthermore, \cite{rle-paper} conclude based on figure 9 page 9 that the conditioning of the policy network on the latent vector is important. However, in this figure \textsc{RLE} without latent vector conditioning shows similar performance to \textsc{RLE} with latent vector conditioning in three out of four \textsc{Atari} games. Only in one game the latent vector conditioned \textsc{RLE} version performs better. Thus, the importance of the latent vector conditioning should be further investigated.

We have also taken up the idea of \cite{rle-paper} to change the latent vector distribution over time by implementing the queue-based adaptive VMF and neural-adaptive VMF \textsc{RLE} variants. Both variants outperformed standard \textsc{RLE} in the \textsc{Atari} game \textsc{Alien}. It is worthwhile to explore how these \textsc{RLE} variants perform in other \textsc{Atari} games and in the various \textsc{IsaacLab} environments.

\subsection{What was easy}

\noindent As we chose an RL paper to reproduce, we did not have to deal with the hassle of downloading huge data sets for training. Moreover, for us the concept of the paper, especially the new idea introduced by \cite{rle-paper}, was fairly straight-forward and easy to understand. Moreover, the code provided by the authors for the \textsc{Atari} environment was comparatively ready to run, we did not have to make a lot of modifications here.

\subsection{What was difficult}

\noindent The largest challenges were the lack of computational resources, time limitations. Moreover, the paper is missing a proper description of the RLE architecture, as well as there not being a good \textsc{IsaacLab} documentation. For more details, see \hyperlink{appendix-difficult}{appendix I}.

\subsection{Communication with original authors}

\noindent During our replication study, a number of questions, mainly regarding parameters and model/environment architecture, came up which we were unable to answer by ourselves, so we sent an email to Mr. Mahankali. and got a response a few days later. Our questions and Mr. Mahankali's answers will be provided upon request. 

\section{Conclusion}

\noindent From our experiments it became evident that RLE is a suitable alternative to RND and NoisyNet for RL settings, especially in the case where deep exploration is necessary. However, due to limited computational resources and time constraints we could not test all of the authors' claims but what we found is roughly in line with their results. For future efforts it may be rewarding to test all the other \textsc{Atari} games and \textsc{Isaac} evnironments to further solidify the findings. Furthermore, the adaptive VMF techniques showed promising results in the \textsc{Alien} game and it may be a rewarding challenge to run it on other games as well and test it in the continuous action-space domain.

\clearpage
\section*{Member contributions}

\noindent We divided the workload among the group members as follows:

\begin{itemize}
  \item Jonatan Bella: VMF algorithms development and experimentation, \textsc{IsaacLab} implementation (rewriting of each component of the project code, environment modifications for RLE, and final experimentation)
  \item Alessia Berarducci: \textsc{Atari} implementation, IQM / bootstrapped confidence interval (not used in the end due to compute time), Small Reward Compensation Hypothesis.
  \item Jonas Knupp: Implementation of the \textsc{FourRoom} environment, adaption of PPO, NoisyNet, RLE, and RND to the \textsc{FourRoom} environment, conducting the experiments with the \textsc{FourRoom} environment
  \item Tobias Erbacher: Report writing, project management, numpy-visualization of \textsc{Atari} interface, plots, assistance in conceptual questions.
\end{itemize}

\noindent However, we met regularly and assisted each other in their tasks so the split is not completely clear-cut.

%%%%

\bibliography{main}
\bibliographystyle{tmlr}

\include{appendix}

\end{document}
