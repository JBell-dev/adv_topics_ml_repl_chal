\appendix
\section{Pseudocode --- Random Latent Exploration}

\noindent For the RLE algorithm, the detailed pseudocode according to \cite{rle-paper} is as follows:

\hypertarget{algo-rle}{
\begin{table}[h!]
  \centering
  \begin{tabular}{rlllll}
    \hline\hline
    \textbf{RLE:} & \multicolumn{5}{l}{Detailed Pseudocode}\\
    \hline
    1: & \multicolumn{5}{l}{\textbf{Input:} Latent distribution $\left(P_{\textbf{z}}\right)$, number of parallel workers $\left(N\right)$, number of steps per update}\\
    & \multicolumn{5}{l}{$\left(T\right)$, number of steps per sampling $\left(S\right)$, and feature network update rate $\left(\tau\right)$.}\\
    2: & \multicolumn{5}{l}{Randomly initialize a feature network $\left(\phi\right)$ with the same backbone architecture as the policy }\\
    & \multicolumn{5}{l}{and value networks.}\\
    3: & \multicolumn{5}{l}{Initialize running mean $\boldsymbol{\mu} = \textbf{0}$ and standard deviation $\boldsymbol{\sigma} = \textbf{1}$ estimates of $\boldsymbol{\phi} \left(s\right)$ over the state}\\
    & \multicolumn{5}{l}{space.}\\
    4: & \multicolumn{5}{l}{Sample an initial latent vector $\textbf{z} \sim P_{\textbf{z}}$ for each parallel worker.}\\
    5: & \multicolumn{5}{l}{\textbf{Repeat (\textsuperscript{1})}}\\
    6: & | & \multicolumn{4}{l}{Sample initial state $s_{0}$.}\\
    7: & | & \multicolumn{4}{l}{\textbf{For} $t = 0, ..., T$ \textbf{do (\textsuperscript{2})}}\\
    8: & | & | & \multicolumn{3}{l}{Take action $a_{t} \sim \boldsymbol{\pi}(. | s_{t}, \textbf{z})$ and transition to $s_{t+1}$.}\\
    9: & | & | & \multicolumn{3}{l}{Compute feature $\textbf{f}(s_{t+1}) = \frac{\boldsymbol{\phi}(s_{t+1}) - \boldsymbol{\mu}}{\boldsymbol{\sigma}}$.}\\
    10: & | & | & \multicolumn{3}{l}{Compute random reward $F(s_{t+1}, \textbf{z}) = \frac{\textbf{f}(s_{t+1})}{\| f(s_{t+1}) \|} \cdot \textbf{z}$.}\\
    11: & | & | & \multicolumn{3}{l}{Receive reward $r_{t} = R(s_{t}, a_{t}) + F(s_{t+1}, \textbf{z})$.}\\
    12: & | & | & \multicolumn{3}{l}{\textbf{For} $i = 0, 1, ..., N-1$ \textbf{do (\textsuperscript{3})}}\\
    13: & | & | & | & \multicolumn{2}{l}{\textbf{If} worker $i$ terminated \textbf{or} $S$ timesteps passed without resampling, \textbf{then (\textsuperscript{4})}}\\
    14: & | & | & | & | & Resample sample $\textbf{z} \sim P_{\textbf{z}}$ for worker $i$.\\
    15: & | & | & | & \multicolumn{2}{l}{\textbf{(\textsuperscript{4})}}\\
    16: & | & | & \multicolumn{3}{l}{\textbf{(\textsuperscript{3})}}\\
    17: & | & \multicolumn{4}{l}{\textbf{(\textsuperscript{2})}}\\
    18: & | & \multicolumn{4}{l}{Update policy network $\boldsymbol{\pi}$ and value network $V_{\boldsymbol{\pi}}$ with the collected trajectory}\\
    & | & \multicolumn{4}{l}{$(\textbf{z}, s_{0}, a_{0}, r_{0}, s_{1}, ..., s_{T})$.}\\
    19: & | & \multicolumn{4}{l}{Update feature network $\boldsymbol{\phi}$ using the value network's parameters $\boldsymbol{\phi} \leftarrow \tau \cdot V^{\boldsymbol{\pi}} + (1 - \tau) \cdot \boldsymbol{\phi}$.}\\
    20: & | & \multicolumn{4}{l}{Update $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ using the batch of collected experience.}\\
    21: & \multicolumn{5}{l}{\textbf{(\textsuperscript{1}) until convergence}}\\
    \hline\hline
  \end{tabular}
\end{table}}

\section{Pseudocode --- Proximal Policy Optimization}

\noindent For the PPO algorithm, the detailed pseudocode according to \cite{ppo-paper} is as follows:

\hypertarget{algo-ppo}{
\begin{table}[h!]
  \centering
  \begin{tabular}{rlll}
    \hline\hline
    \textbf{PPO:} & \multicolumn{3}{l}{Detailed Pseudocode, Actor-Critic Setup}\\
    \hline
    1: & \multicolumn{3}{l}{\textbf{Input:} Number of iterations $\left(I\right)$, number of actors $\left(N\right)$, number of timesteps per iteration}\\
    & \multicolumn{3}{l}{$\left(T\right)$, number of epochs $(K)$, minibatch size $(M \leq NT)$, learned state-value function implicitly}\\
    & \multicolumn{3}{l}{in the advantage estimator $(V(s))$, target value $(V_{t}^{\text{targ}})$ implicitly in the objective $L$.}\\
    2: & \multicolumn{3}{l}{\textbf{For} $i = 1, ..., I$ \textbf{do (\textsuperscript{1})}}\\
    3: & | & \multicolumn{2}{l}{\textbf{For} $n = 1, ..., N$ \textbf{do (\textsuperscript{2})}}\\
    4: & | & | & Run policy $\boldsymbol{\pi}_{\theta_{\text{old}}}$ in environment for $T$ timesteps.\\
    5: & | & | & Compute advantage estimates $\hat{A}_{1}, ..., \hat{A}_{T}$.\\
    6: & | & \multicolumn{2}{l}{\textbf{(\textsuperscript{2})}}\\
    7: & | & \multicolumn{2}{l}{Optimize objective $L$ w.r.t. $\theta$, for given $K$ and $M$, using e.g. minibatch SGD or Adam.}\\
    8: & | & \multicolumn{2}{l}{Update parameters $\theta_{\text{old}} \leftarrow \theta$.}\\
    9: & \multicolumn{3}{l}{\textbf{(\textsuperscript{1})}}\\
    \hline\hline
  \end{tabular}
\end{table}}

\clearpage
\section{Pseudocode --- NoisyNet}

\noindent For the NoisyNet + A3C algorithm, the detailed pseudocode according to \cite{noisynet-paper} is as follows:

\hypertarget{algo-noisynet}{
\begin{table}[h!]
  \centering
  \begin{tabular}{rlll}
    \hline\hline
    \textbf{NoisyNet:} & \multicolumn{3}{l}{Detailed Pseudocode, for each actor-learner thread}\\
    \hline
    1: & \multicolumn{3}{l}{\textbf{Input:} Global shared parameters $(\zeta_{\pi}, \zeta_{V})$, global shared counter $(T)$, and maximal}\\
    & \multicolumn{3}{l}{time $(T_{\text{max}})$.}\\
    2: & \multicolumn{3}{l}{\textbf{Input (each thread):} Thread-specific parameters $(\zeta_{\pi}', \zeta_{V}')$, set of random variables $(\varepsilon)$,}\\
    & \multicolumn{3}{l}{thread-specific counter $(t)$, and roll-out size $(t_{\text{max}})$.}\\
    3: & \multicolumn{3}{l}{\textbf{Output:} Policy $\pi(\cdot|\zeta_{\pi}, \varepsilon)$ and value function $V(\cdot|\zeta_{V},\varepsilon)$.}\\
    4: & \multicolumn{3}{l}{$t \leftarrow 1$}\\
    5: & \multicolumn{3}{l}{\textbf{Repeat (\textsuperscript{1})}}\\
    6: & | & \multicolumn{2}{l}{Reset cumulative gradients: $d\zeta_{\pi} \leftarrow 0, d\zeta_{V} \leftarrow 0$.}\\
    7: & | & \multicolumn{2}{l}{Synchronize thread-specific parameters: $\zeta_{\pi}' \leftarrow \zeta_{\pi}, \zeta_{V}' \leftarrow \zeta_{V}$.}\\
    8: & | & \multicolumn{2}{l}{Set counter $c \leftarrow 0$.}\\
    9: & | & \multicolumn{2}{l}{Get state $x_{t}$ from environment.}\\
    10: & | & \multicolumn{2}{l}{Sample noise $\xi \sim \varepsilon$.}\\
    11: & | & \multicolumn{2}{l}{Create lists for rewards $r \leftarrow [ ]$, actions $a \leftarrow [ ]$ and states $x \leftarrow [ ]$.}\\
    12: & | & \multicolumn{2}{l}{\textbf{Repeat (\textsuperscript{2})}}\\
    13: & | & | & Choose action $a_{t} \leftarrow \pi(\cdot|x_{t}, \zeta_{pi}', \zeta_{V}')$.\\
    14: & | & | & $a[-1] \leftarrow a_{t}$\\
    15: & | & | & Obtain reward $r_{t}$ and new state $x_{t+1}$.\\
    16: & | & | & $r[-1] \leftarrow r_{t}$, $x[-1] \leftarrow x_{t}$, $t \leftarrow t + 1$, $T \leftarrow T + 1$, $c \leftarrow c + 1$\\
    17: & | & \multicolumn{2}{l}{\textbf{(\textsuperscript{2}) until} $x_{t}$ \textbf{is terminal or} $c > t_{\text{max}}$}\\
    18: & | & \multicolumn{2}{l}{\textbf{If} $x_{t}$ is terminal, \textbf{then (\textsuperscript{3})}}\\
    19: & | & | & $Q \leftarrow 0$\\
    20: & | & \multicolumn{2}{l}{\textbf{else}}\\
    21: & | & | & $Q \leftarrow V\left(x_{t}|\zeta_{V}', \xi\right)$\\
    22: & | & \multicolumn{2}{l}{\textbf{(\textsuperscript{3})}}\\
    23: & | & \multicolumn{2}{l}{\textbf{For} $i = c - 1, ..., 0$ \textbf{do (\textsuperscript{4})}}\\
    24: & | & | & $Q \leftarrow r\left[i\right] + \gamma Q$\\
    25: & | & | & $d\zeta_{\pi} \leftarrow d\zeta_{\pi} \nabla_{\zeta_{\pi}'} \log{(\pi(a[i]|x[i], \zeta_{\pi}', \xi)) [Q - V(x[i]|\zeta_{V}', \xi)]}$\\
    26: & | & | & $d\zeta_{V} \leftarrow d\zeta_{V} \nabla_{\zeta_{V}'} \left[Q - V\left(x[i]|\zeta_{V}', \xi\right)\right]^{2}$\\
    27: & | & \multicolumn{2}{l}{\textbf{(\textsuperscript{4})}}\\
    28: & | & \multicolumn{2}{l}{Update asynchronously parameter $\zeta_{\pi} \leftarrow \zeta_{\pi} + \alpha_{\pi}d\zeta_{\pi}$.}\\
    29: & | & \multicolumn{2}{l}{Update asynchronously parameter $\zeta_{V} \leftarrow \zeta_{V} - \alpha_{V}d\zeta_{V}$.}\\
    30: & \multicolumn{3}{l}{\textbf{(\textsuperscript{1}) until} $T > T_{\text{max}}$}\\
    \hline\hline
  \end{tabular}
\end{table}}

\clearpage
\section{Pseudocode --- Random Network Distillation}

\noindent For the RND algorithm, the detailed pseudocode according to \cite{rnd-paper} is as follows:

\hypertarget{algo-rnd}{
\begin{table}[h!]
  \centering
  \begin{tabular}{rlll}
    \hline\hline
    \textbf{RND:} & \multicolumn{3}{l}{Detailed Pseudocode}\\
    \hline
    1: & \multicolumn{3}{l}{\textbf{Input:} Number of rollouts $(N)$, Number of optimization steps $(N_{\text{opt}})$, and length of initial}\\
    & \multicolumn{3}{l}{steps for initializing observation normalization $(M)$.}\\
    2: & \multicolumn{3}{l}{$t \leftarrow 0$}\\
    3: & \multicolumn{3}{l}{Sample state $s_{0} \sim p_{0}(s_{0})$.}\\
    4: & \multicolumn{3}{l}{\textbf{For} $m = 1, ..., M$ \textbf{do (\textsuperscript{1})}}\\
    5: & | & \multicolumn{2}{l}{Sample action $a_{t} \sim \mathcal{U}(a_{t})$. \textcolor{red}{Are we sampling uniformly from all available actions at time $t$?}}\\
    6: & | & \multicolumn{2}{l}{Sample state $s_{t+1} \sim p(s_{t+1}|s_{t}, a_{t})$.}\\
    7: & | & \multicolumn{2}{l}{Update observation normalization parameters using $s_{t+1}$.}\\
    8: & | & \multicolumn{2}{l}{$t \leftarrow t + 1$}\\
    9: & \multicolumn{3}{l}{\textbf{(\textsuperscript{1})}}\\
    10: & \multicolumn{3}{l}{\textbf{For} $i = 1, ..., N$ \textbf{do (\textsuperscript{2})}}\\
    11: & | & \multicolumn{2}{l}{\textbf{For} $j = 1, ..., K$ \textbf{do (\textsuperscript{3})}}\\
    12: & | & | & Sample action $a_{t} \sim \pi(a_{t}|s_{t})$.\\
    13: & | & | & Sample state $s_{t+1}, e_{t} \sim p(s_{t+1}, e_{t}|s_{t}, a_{t})$.\\
    14: & | & | & Calculate intrinsic reward $i_{t} = \|\hat{f}(s_{t+1}) - f(s_{t+1})\|^{2}$.\\
    15: & | & | & Add $s_{t}, s_{t+1}, a_{t}, e_{t}, i_{t}$ to optimization batch $B_{i}$.\\
    16: & | & | & Update running estimate of reward standard deviation using $i_{t}$.\\
    17: & | & | & $t \leftarrow t + 1$\\
    18: & | & \multicolumn{2}{l}{\textbf{(\textsuperscript{3})}}\\
    19: & | & \multicolumn{2}{l}{Normalize the intrinsic rewards contained in $B_{i}$.}\\
    20: & | & \multicolumn{2}{l}{Calculate returns $R_{I,i}$ and advantages $A_{I,i}$ for intrinsic reward.}\\
    21: & | & \multicolumn{2}{l}{Calculate returns $R_{E,i}$ and advantages $A_{E,i}$ for extrinsic reward.}\\
    22: & | & \multicolumn{2}{l}{Calculate combined advantages $A_{i} = A_{I,i} + A_{E,i}$.}\\
    23: & | & \multicolumn{2}{l}{Update observation normalization parameters using $B_{i}$.}\\
    24: & | & \multicolumn{2}{l}{\textbf{For} $j = 1, ..., N_{\text{opt}}$ \textbf{do (\textsuperscript{4})}}\\
    25: & | & | & Optimize $\theta_{\pi}$ w.r.t. PPO loss on batch $B_{i}, R_{i}, A_{i}$ using Adam.\\
    26: & | & | & Optimize $\theta_{\hat{f}}$ w.r.t. distillation loss on $B_{i}$ using Adam.\\
    27: & | & \multicolumn{2}{l}{\textbf{(\textsuperscript{4})}}\\
    28: & \multicolumn{3}{l}{\textbf{(\textsuperscript{2})}}\\
    \hline\hline
  \end{tabular}
\end{table}}

\section{Citations, figures, and tables}

\paragraph{Citations}

When the authors or the publication are
included in the sentence, the citation should not be in parenthesis, using \verb|\citet{}| (as
in ``See (CITATION REMOVED) for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~(CITATION REMOVED).'').

\paragraph{Figures}

All artwork must be neat, clean, and legible. The figure number and caption always appear after the figure. Place one line space before the figure caption, and one line space after the figure. The figure caption is lowercase (except for the first word and proper nouns). Make sure the figure caption does not get separated from the figure. Leave sufficient space to avoid splitting the figure and figure caption.

\begin{figure}[ht]
\begin{center}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\paragraph{Tables}

All tables must be centered, neat, clean and legible. The table number and title always appear before the table. See Table~\ref{sample-table}. Place one line space before the table title, one line space after the table title, and one line space after the table. The table title must be lowercase (except for the first word and proper nouns).

\begin{table}[ht]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Math Notation}

In an attempt to encourage standardized notation, we have included the
notation file from the textbook, \textit{Deep Learning}
(CITATION REMOVED) available at
\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
is not required and can be disabled by commenting out
\texttt{math\_commands.tex}.
\clearpage

\centerline{\bf Numbers and Arrays}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1in}p{3.25in}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix\\
$\displaystyle \tA$ & A tensor\\
$\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
$\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
$\displaystyle \ra$ & A scalar random variable\\
$\displaystyle \rva$ & A vector-valued random variable\\
$\displaystyle \rmA$ & A matrix-valued random variable\\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Sets and Graphs}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \sA$ & A set\\
$\displaystyle \R$ & The set of real numbers \\
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
$\displaystyle \gG$ & A graph\\
$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\egroup
\vspace{0.25cm}

\clearpage
\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
$\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
$\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
$\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
$\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
$\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
$\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\vspace{0.25cm}


\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
$\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
$\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
$\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\clearpage
\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
  $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
  (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log x$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
$\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
$\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
$\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
$\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
\end{tabular}
\egroup
\vspace{0.25cm}
